{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5344: Spark lab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Li Jiazhe (A0176576M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Input: a set of files under a directory\n",
    "\n",
    "• Compute frequency of every word in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"WordCounter\")\n",
    "         .set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/spark/Downloads/lab1/stopwords.txt\",\"r\",encoding=\"utf-8\")\n",
    "lines = f.readlines()\n",
    "stopwords = [x.strip() for x in lines] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def process_words(context): # leave out numbers and punctuations and split words\n",
    "        path = context[0] \n",
    "        prewords = re.sub('[^a-z]+',' ',context[1].lower()).split() \n",
    "        words = [word for word in prewords if word not in stopwords]\n",
    "        file = path.split(\"/\")[-1] # get the file name from path\n",
    "      \n",
    "        # appending the file name\n",
    "        list = [x + '@' + file for x in words]\n",
    "        return list\n",
    "\n",
    "    text_files = sc.wholeTextFiles(\"/home/spark/Downloads/lab1/datafiles/*\")\n",
    "    word_doc_count = text_files.flatMap(process_words)\\\n",
    "                       .map(lambda w: (w,1)) \\\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                \n",
    "    docs_count = len(text_files.collect()) # total number of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Input: Stage 1 output\n",
    "\n",
    "• Compute TF-IDF of every word w.r.t a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def word_docid_count(val): # input word_doc_count (k-v pair:\"word@doc:count\"), output k-v pair:\"word:docid@count\"\n",
    "        word_docid, count = val \n",
    "        word, docid = word_docid.split('@')  \n",
    "        return (word, '{0}@{1}'.format(docid, count))\n",
    "\n",
    "    def word_docid_count_docswithword(val): # input k-v pair:\"word:docid@count\", output k-v pair:\"word@doc:count@counter\n",
    "        word = val[0]\n",
    "        docid_count_pairs = []\n",
    "        counter = 0\n",
    "        for docid_count in val[1]: \n",
    "            counter = counter + 1 # after group by key, count the number of appearance of the key so that  \n",
    "                                  # we get the number of docs that contains the key word.\n",
    "            docid, count = docid_count.split('@')\n",
    "            docid_count_pairs.append((docid, count)) # further split the value to a list with pairs for calculation\n",
    "        \n",
    "        result = []\n",
    "        for (docid, count) in docid_count_pairs:\n",
    "            word_docid = '{0}@{1}'.format(word, docid)\n",
    "            count_docswithword = '{0}@{1}'.format(count, counter)\n",
    "            result.append((word_docid, count_docswithword)) \n",
    "        return result\n",
    "\n",
    "    def count_tf_idf(val): # input k-v pair:\"word@doc:count@counter\", output k-v pair:\"word@doc:tfidf\"\n",
    "        word_docid, count_docswithword = val\n",
    "        fdt, dft = [int(x) for x in count_docswithword.split('@')]\n",
    "        # fdt: number of the word appears in the doc; dft: counter\n",
    "        return (word_docid, (1 + math.log(fdt)) * math.log(docs_count / dft))\n",
    "    \n",
    "    tfidf_raw = word_doc_count.map(word_docid_count)\\\n",
    "                              .groupByKey()\\\n",
    "                              .flatMap(word_docid_count_docswithword)\\\n",
    "                              .map(count_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Input: Stage 2 output\n",
    "\n",
    "• Compute normalized TF-IDF of every word w.r.t. a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def docid_word_tfidf(val): # input tfidf (k-v pair:\"word@doc:tfidf\"), output k-v pair:\"docid:word@tfidf\"\n",
    "        word_docid, tfidf = val # word_docid:\"word@doc\n",
    "        word, docid = word_docid.split('@')  \n",
    "        return (docid, '{0}@{1}'.format(word, tfidf))\n",
    "\n",
    "    def docid_word_sos_tfidf(val): # input k-v pair:\"docid:word@tfidf\", output k-v pair:\"word@doc:tfidf@sos_tfidf\n",
    "        docid = val[0]\n",
    "        word_tfidf_pairs = []\n",
    "        sos_tfidf = 0 # sum of square tfidf\n",
    "        for word_tfidf in val[1]: \n",
    "            word, tfidf = word_tfidf.split('@')\n",
    "            sos_tfidf = sos_tfidf + float(tfidf) ** 2\n",
    "            word_tfidf_pairs.append((word, tfidf)) \n",
    "        sqrt_sos_tfidf = math.sqrt(sos_tfidf)\n",
    "        \n",
    "        result = []\n",
    "        for (word, tfidf) in word_tfidf_pairs:\n",
    "            word_docid = '{0}@{1}'.format(word, docid)\n",
    "            tfidf_sqrt_sos_tfidf = '{0}@{1}'.format(tfidf, sqrt_sos_tfidf)\n",
    "            result.append((word_docid, tfidf_sqrt_sos_tfidf)) \n",
    "        return result\n",
    "\n",
    "    def norm_tf_idf(val): # input k-v pair:\"word@doc:tfidf@sos_tfidf\", output k-v pair:\"word@doc:n_tfidf\n",
    "        word_docid, tfidf_sqrt_sos_tfidf = val\n",
    "        tfidf, sqrt_sos_tfidf = [float(x) for x in tfidf_sqrt_sos_tfidf.split('@')]\n",
    "        return (word_docid, tfidf/sqrt_sos_tfidf)\n",
    "\n",
    "    norm_tfidf = tfidf_raw.map(docid_word_tfidf)\\\n",
    "                          .groupByKey()\\\n",
    "                          .flatMap(docid_word_sos_tfidf)\\\n",
    "                          .map(norm_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Input: Stage 4 output and a query file query.txt\n",
    "\n",
    "• Compute relevance of every document w.r.t a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = open(\"/home/spark/Downloads/lab1/query.txt\",\"r\",encoding=\"utf-8\")\n",
    "query = q.read().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    def docid_n_tfidf_words(val): # input k-v pair:\"word@doc:n_tfidf\", output k-v pair:\"doc@n_tfidf:word\"\n",
    "        word_docid, n_tfidf = val\n",
    "        word, docid = word_docid.split('@') \n",
    "        return ('{0}@{1}'.format(docid, n_tfidf), word)\n",
    "    \n",
    "    def docid_word_norm_tfidf(val): # input k-v pair:\"doc@n_tfidf:word\", output k-v pair:\"docid:n_tfidf\"\n",
    "        docid_n_tfidf, word = val \n",
    "        docid, n_tfidf = docid_n_tfidf.split('@')  \n",
    "        return (docid, float(n_tfidf))\n",
    "    \n",
    "    relevance = norm_tfidf.map(docid_n_tfidf_words)\\\n",
    "                          .filter(lambda x: x[1] in query)\\\n",
    "                          .map(docid_word_norm_tfidf)\\\n",
    "                          .reduceByKey(lambda x,y: x+y)\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Input: Stage 4 output\n",
    "\n",
    "• Sort documents by their relevance to the query in descending order\n",
    "\n",
    "• Output the top-k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('f4.txt', 0.05016228052691914), ('f8.txt', 0.04617866352263805), ('f1.txt', 0.03310038645394255), ('f3.txt', 0.0242424688450231), ('f2.txt', 0.018215906118135144), ('f5.txt', 0.017954895073001197), ('f10.txt', 0.011373054107967922)]\n"
     ]
    }
   ],
   "source": [
    "sorted_relevance = relevance.sortBy(lambda x: x[1], ascending=False)\n",
    "                    \n",
    "print(sorted_relevance.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sequence of relevance to the query is f4 > f8 > f1 > f3 > f2 > f5 > f10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
